	.file	"inverse.c"
	.text
	.p2align 4
	.globl	inverse
	.type	inverse, @function
inverse:
.LFB0:
	.cfi_startproc
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movsd	24(%rdi), %xmm5
	movsd	48(%rdi), %xmm2
	movsd	(%rdi), %xmm6
	movsd	8(%rdi), %xmm8
	movapd	%xmm5, %xmm1
	movsd	32(%rdi), %xmm0
	movsd	16(%rdi), %xmm7
	movapd	%xmm5, %xmm13
	mulsd	%xmm2, %xmm1
	movapd	%xmm6, %xmm11
	movapd	%xmm8, %xmm10
	movsd	40(%rdi), %xmm4
	mulsd	%xmm7, %xmm11
	movapd	%xmm7, %xmm14
	movapd	%xmm8, %xmm12
	movsd	56(%rdi), %xmm3
	mulsd	%xmm7, %xmm10
	mulsd	%xmm0, %xmm14
	mulsd	%xmm0, %xmm13
	movsd	%xmm1, -112(%rsp)
	mulsd	%xmm0, %xmm12
	movapd	%xmm6, %xmm1
	mulsd	%xmm8, %xmm1
	movsd	%xmm10, -32(%rsp)
	movsd	%xmm14, -80(%rsp)
	movapd	%xmm10, %xmm14
	movsd	%xmm13, -120(%rsp)
	mulsd	%xmm2, %xmm14
	movapd	%xmm1, %xmm9
	mulsd	%xmm0, %xmm9
	movsd	%xmm14, -64(%rsp)
	movapd	%xmm7, %xmm14
	mulsd	%xmm5, %xmm14
	movsd	%xmm9, -104(%rsp)
	movapd	%xmm11, %xmm9
	mulsd	%xmm0, %xmm11
	movapd	%xmm9, %xmm13
	mulsd	%xmm2, %xmm13
	movsd	%xmm14, -24(%rsp)
	movsd	%xmm11, -96(%rsp)
	movapd	%xmm10, %xmm11
	movapd	%xmm6, %xmm10
	mulsd	%xmm5, %xmm10
	movsd	%xmm13, -72(%rsp)
	movapd	%xmm9, %xmm13
	mulsd	%xmm5, %xmm13
	mulsd	%xmm4, %xmm11
	movapd	%xmm10, %xmm15
	mulsd	%xmm2, %xmm15
	movsd	%xmm11, -88(%rsp)
	movapd	%xmm14, %xmm11
	mulsd	%xmm0, %xmm14
	mulsd	%xmm2, %xmm11
	movsd	%xmm15, -56(%rsp)
	movapd	%xmm8, %xmm15
	mulsd	%xmm5, %xmm15
	movsd	%xmm14, (%rsp)
	movsd	%xmm11, -48(%rsp)
	movapd	%xmm1, %xmm11
	mulsd	%xmm5, %xmm11
	movsd	%xmm15, -16(%rsp)
	movapd	%xmm13, %xmm15
	mulsd	%xmm0, %xmm13
	movsd	%xmm13, -40(%rsp)
	movapd	%xmm15, %xmm13
	mulsd	%xmm2, %xmm15
	mulsd	%xmm4, %xmm13
	movsd	%xmm15, 8(%rsp)
	movapd	%xmm1, %xmm15
	mulsd	%xmm7, %xmm15
	movsd	%xmm13, -8(%rsp)
	movsd	-40(%rsp), %xmm13
	mulsd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm13
	movapd	%xmm15, %xmm14
	movsd	%xmm15, 16(%rsp)
	mulsd	%xmm0, %xmm14
	mulsd	%xmm5, %xmm15
	movsd	%xmm13, 32(%rsp)
	movapd	%xmm12, %xmm13
	mulsd	%xmm4, %xmm13
	mulsd	%xmm4, %xmm5
	movsd	%xmm14, 24(%rsp)
	movsd	%xmm15, 40(%rsp)
	movapd	%xmm15, %xmm14
	movapd	%xmm6, %xmm15
	addsd	%xmm0, %xmm15
	mulsd	%xmm0, %xmm14
	mulsd	%xmm2, %xmm5
	addsd	%xmm12, %xmm15
	movsd	%xmm14, 48(%rsp)
	addsd	%xmm7, %xmm15
	movapd	%xmm4, %xmm7
	addsd	-112(%rsp), %xmm15
	movsd	-120(%rsp), %xmm14
	mulsd	%xmm2, %xmm7
	addsd	%xmm7, %xmm15
	movapd	%xmm6, %xmm7
	mulsd	%xmm3, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm0, %xmm15
	mulsd	%xmm3, %xmm15
	addsd	%xmm15, %xmm7
	addsd	-104(%rsp), %xmm7
	movapd	%xmm13, %xmm15
	addsd	-96(%rsp), %xmm7
	addsd	-88(%rsp), %xmm7
	movapd	%xmm14, %xmm13
	mulsd	%xmm4, %xmm13
	addsd	%xmm7, %xmm15
	movsd	-80(%rsp), %xmm7
	mulsd	%xmm4, %xmm7
	addsd	%xmm15, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm12, %xmm13
	addsd	-72(%rsp), %xmm7
	mulsd	%xmm2, %xmm13
	addsd	-64(%rsp), %xmm7
	addsd	-56(%rsp), %xmm7
	addsd	-48(%rsp), %xmm7
	mulsd	%xmm3, %xmm12
	movapd	%xmm13, %xmm15
	movapd	%xmm6, %xmm13
	addsd	%xmm7, %xmm15
	mulsd	%xmm4, %xmm13
	movapd	%xmm14, %xmm7
	mulsd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm6
	addsd	%xmm15, %xmm7
	movapd	%xmm13, %xmm15
	movsd	-120(%rsp), %xmm13
	mulsd	%xmm2, %xmm15
	mulsd	%xmm3, %xmm13
	mulsd	%xmm3, %xmm6
	addsd	%xmm7, %xmm15
	movapd	%xmm8, %xmm7
	mulsd	%xmm4, %xmm7
	mulsd	%xmm2, %xmm8
	mulsd	%xmm2, %xmm7
	mulsd	%xmm3, %xmm8
	addsd	%xmm15, %xmm7
	movsd	-16(%rsp), %xmm15
	addsd	%xmm7, %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm4, %xmm7
	mulsd	%xmm2, %xmm7
	addsd	%xmm7, %xmm5
	movapd	%xmm9, %xmm7
	mulsd	%xmm3, %xmm7
	mulsd	%xmm4, %xmm9
	addsd	%xmm7, %xmm5
	movapd	%xmm10, %xmm7
	mulsd	%xmm3, %xmm7
	mulsd	%xmm2, %xmm9
	addsd	%xmm5, %xmm7
	movapd	%xmm15, %xmm5
	mulsd	%xmm3, %xmm5
	addsd	%xmm7, %xmm5
	movsd	-104(%rsp), %xmm7
	addsd	%xmm5, %xmm12
	movsd	-80(%rsp), %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm14
	addsd	%xmm12, %xmm14
	movapd	%xmm11, %xmm12
	mulsd	%xmm4, %xmm12
	addsd	%xmm14, %xmm13
	movsd	-8(%rsp), %xmm14
	addsd	%xmm13, %xmm6
	movsd	(%rsp), %xmm13
	addsd	%xmm6, %xmm8
	movsd	-112(%rsp), %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm7, %xmm6
	addsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm6
	movapd	%xmm0, %xmm8
	mulsd	%xmm2, %xmm8
	mulsd	%xmm3, %xmm8
	addsd	%xmm5, %xmm8
	movapd	%xmm11, %xmm5
	mulsd	%xmm0, %xmm5
	addsd	%xmm5, %xmm8
	addsd	-40(%rsp), %xmm8
	movapd	%xmm13, %xmm5
	mulsd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm13
	addsd	%xmm12, %xmm8
	movapd	%xmm11, %xmm12
	mulsd	%xmm2, %xmm12
	mulsd	%xmm3, %xmm11
	addsd	%xmm14, %xmm8
	addsd	%xmm8, %xmm6
	movsd	8(%rsp), %xmm8
	addsd	%xmm6, %xmm5
	movsd	-96(%rsp), %xmm6
	mulsd	%xmm2, %xmm6
	addsd	%xmm12, %xmm5
	movapd	%xmm10, %xmm12
	mulsd	%xmm0, %xmm12
	mulsd	%xmm4, %xmm10
	addsd	%xmm8, %xmm5
	addsd	%xmm6, %xmm5
	movsd	-32(%rsp), %xmm6
	mulsd	%xmm2, %xmm10
	mulsd	%xmm0, %xmm6
	mulsd	%xmm15, %xmm0
	mulsd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm0
	addsd	%xmm5, %xmm6
	movapd	%xmm12, %xmm5
	movapd	%xmm15, %xmm12
	mulsd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm12
	addsd	%xmm6, %xmm5
	movapd	%xmm1, %xmm6
	mulsd	%xmm4, %xmm6
	mulsd	%xmm2, %xmm1
	addsd	%xmm5, %xmm0
	movapd	%xmm6, %xmm5
	movsd	40(%rsp), %xmm6
	mulsd	%xmm2, %xmm5
	mulsd	%xmm3, %xmm1
	addsd	%xmm0, %xmm5
	movsd	-88(%rsp), %xmm0
	mulsd	%xmm2, %xmm0
	addsd	%xmm5, %xmm9
	movsd	-24(%rsp), %xmm5
	mulsd	%xmm4, %xmm5
	addsd	%xmm9, %xmm0
	movsd	-72(%rsp), %xmm9
	mulsd	%xmm3, %xmm9
	addsd	%xmm0, %xmm10
	mulsd	%xmm2, %xmm5
	movapd	%xmm15, %xmm0
	mulsd	%xmm4, %xmm0
	mulsd	%xmm2, %xmm0
	addsd	%xmm10, %xmm0
	addsd	%xmm0, %xmm5
	movapd	%xmm7, %xmm0
	movsd	24(%rsp), %xmm7
	mulsd	%xmm3, %xmm0
	addsd	%xmm5, %xmm11
	movapd	%xmm13, %xmm5
	movsd	32(%rsp), %xmm13
	addsd	%xmm11, %xmm0
	addsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	movsd	16(%rsp), %xmm5
	addsd	%xmm1, %xmm0
	movsd	-64(%rsp), %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm9, %xmm0
	addsd	%xmm1, %xmm0
	movsd	-56(%rsp), %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm12, %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movsd	-48(%rsp), %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm7, %xmm1
	mulsd	%xmm4, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm6, %xmm1
	mulsd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm6
	addsd	%xmm13, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm5, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm2, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm14, %xmm1
	mulsd	%xmm2, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm7, %xmm1
	movsd	48(%rsp), %xmm7
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm14, %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm5, %xmm1
	mulsd	%xmm2, %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm8, %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm7, %xmm1
	mulsd	%xmm2, %xmm1
	mulsd	%xmm4, %xmm7
	addsd	%xmm1, %xmm0
	movapd	%xmm6, %xmm1
	mulsd	%xmm2, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm13, %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movapd	%xmm7, %xmm1
	mulsd	%xmm2, %xmm1
	addsd	%xmm1, %xmm0
	ret
	.cfi_endproc
.LFE0:
	.size	inverse, .-inverse
	.ident	"GCC: (GNU) 11.1.0"
	.section	.note.GNU-stack,"",@progbits
